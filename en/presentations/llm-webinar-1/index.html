<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>How Large Language Models (LLM) Work</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.min.css">
    <!-- Dark theme: night -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/theme/night.min.css" id="theme">
    <!-- Code highlighting style (Monokai) -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/monokai.min.css">

    <!-- Open Sans font -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600&display=swap" rel="stylesheet">

    <style>
        body,
        .reveal {
            font-family: 'Open Sans', sans-serif;
            color: #FFF;
        }

        .reveal .slides section {
            font-size: 0.85em;
        }

        /* Interactive elements */
        .interactive {
            cursor: pointer;
            color: #3BE8B0;
        }

        .hidden-content {
            display: none;
            margin-top: 10px;
            padding: 10px;
            background: rgba(0, 0, 0, 0.6);
            border-radius: 4px;
        }

        .diagram-box {
            border: 2px solid #3BE8B0;
            padding: 1em;
            border-radius: 6px;
            margin-top: 1em;
            text-align: center;
        }

        /* Title on first slide */
        .animated-title {
            font-size: 2em;
            animation: slideInTitle 1s ease forwards;
        }

        @keyframes slideInTitle {
            0% {
                transform: translateY(-50px);
                opacity: 0;
            }

            100% {
                transform: translateY(0);
                opacity: 1;
            }
        }

        /* Emoji blocks */
        .emoji-block {
            font-size: 80px;
            text-align: center;
            margin: 20px 0;
        }

        /* Token styles */
        .token {
            display: inline-block;
            padding: 3px 5px;
            margin: 2px;
            border-radius: 4px;
            font-weight: bold;
            font-size: 0.9em;
            color: #fff;
        }

        /* Colors for each of 11 tokens */
        .tcolor0 {
            background: #cc6666;
        }

        .tcolor1 {
            background: #de935f;
        }

        .tcolor2 {
            background: #f0c674;
        }

        .tcolor3 {
            background: #b5bd68;
        }

        .tcolor4 {
            background: #8abeb7;
        }

        .tcolor5 {
            background: #81a2be;
        }

        .tcolor6 {
            background: #b294bb;
        }

        .tcolor7 {
            background: #c5c8c6;
            color: #000;
        }

        /* Light background, dark text */
        .tcolor8 {
            background: #a3685a;
        }

        .tcolor9 {
            background: #5f819d;
        }

        .tcolor10 {
            background: #85678f;
        }

        /* Styles for Satya Nadella quote */
        .quote {
            display: flex;
            align-items: center;
            gap: 10px;
            font-style: italic;
            margin-top: 20px;
        }

        .quote img {
            width: 60px;
            height: 60px;
            border-radius: 50%;
        }

        .comparison {
            background: rgba(59, 232, 176, 0.15);
            padding: 10px;
            border-radius: 4px;
            margin-top: 10px;
        }

        /* Code styles */
        pre code {
            font-size: 0.75em;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
    </style>
</head>

<body>
    <div class="reveal">
        <div class="slides">

            <!-- Slide 1: Title slide -->
            <section data-transition="slide">
                <h2 class="animated-title">How Large Language Models (LLM) Work</h2>
                <h3>Alexander Efremov, AI Expert</h3>
                <p>Aspirity Company<br>
                    Email: <a href="mailto:ae@aspirity.com" style="color:#3BE8B0;">ae@aspirity.com</a> | Telegram: <a
                        href="https://t.me/sabbah13" target="_blank" style="color:#3BE8B0;">@sabbah13</a></p>
                <div class="emoji-block">ü§ñ</div>
            </section>

            <!-- Parent slide 2.1: LLM Architecture (3 vertical) -->
            <section data-transition="slide">
                <!-- (1) Vertical slide: text part -->
                <section data-transition="slide">
                    <h2>LLM Architecture: Code and Weights</h2>
                    <ul>
                        <li><strong>Models consist of two files:</strong></li>
                        <li><strong>Code file:</strong>
                            <ul>
                                <li>Written in C, for example; handles <em>inference</em></li>
                                <li>Usually contains ~500 lines of code</li>
                            </ul>
                        </li>
                        <li><strong>Parameters file (weights):</strong>
                            <ul>
                                <li>Stores trained coefficients ("settings")</li>
                                <li>Can take up tens/hundreds of gigabytes</li>
                                <li>Example: 1.5 trillion parameters in 16-bit storage ‚Üí ~3 TB of weights</li>
                            </ul>
                        </li>
                    </ul>
                    <div style="display: flex; gap: 40px; margin-top:20px;">
                        <div style="flex:1; text-align:center;">
                            <div class="emoji-block" style="font-size: 60px;">üíª</div>
                            <p>Code</p>
                        </div>
                        <div style="flex:1; text-align:center;">
                            <div class="emoji-block" style="font-size: 60px;">‚öñÔ∏è</div>
                            <p>Weights</p>
                        </div>
                    </div>
                </section>

                <!-- (2) Vertical slide: Llama 3 code example -->
                <section data-transition="slide">
                    <h2>Llama 3 Code Example</h2>
                    <pre><code data-trim data-noescape class="language-python">
# Copyright (c) Meta Platforms, Inc. and affiliates.
# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.

import math
from dataclasses import dataclass
from typing import Optional, Tuple

import fairscale.nn.model_parallel.initialize as fs_init
import torch
import torch.nn.functional as F
from fairscale.nn.model_parallel.layers import (
    ColumnParallelLinear,
    RowParallelLinear,
    VocabParallelEmbedding,
)
from torch import nn


@dataclass
class ModelArgs:
    dim: int = 4096
    n_layers: int = 32
    n_heads: int = 32
    n_kv_heads: Optional[int] = None
    vocab_size: int = -1
    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2
    ffn_dim_multiplier: Optional[float] = None
    norm_eps: float = 1e-5
    rope_theta: float = 500000

    max_batch_size: int = 32
    max_seq_len: int = 2048


class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float()).type_as(x)
        return output * self.weight


def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    t = torch.arange(end, device=freqs.device, dtype=torch.float32)
    freqs = torch.outer(t, freqs)
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64
    return freqs_cis


def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):
    ndim = x.ndim
    assert 0 <= 1 < ndim
    assert freqs_cis.shape == (x.shape[1], x.shape[-1])
    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]
    return freqs_cis.view(*shape)


def apply_rotary_emb(
    xq: torch.Tensor,
    xk: torch.Tensor,
    freqs_cis: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor]:
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xk)


def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """torch.repeat_interleave(x, dim=2, repeats=n_rep)"""
    bs, slen, n_kv_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    return (
        x[:, :, :, None, :]
        .expand(bs, slen, n_kv_heads, n_rep, head_dim)
        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)
    )


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
        model_parallel_size = fs_init.get_model_parallel_world_size()
        self.n_local_heads = args.n_heads // model_parallel_size
        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size
        self.n_rep = self.n_local_heads // self.n_local_kv_heads
        self.head_dim = args.dim // args.n_heads

        self.wq = ColumnParallelLinear(
            args.dim,
            args.n_heads * self.head_dim,
            bias=False,
            gather_output=False,
            init_method=lambda x: x,
        )
        self.wk = ColumnParallelLinear(
            args.dim,
            self.n_kv_heads * self.head_dim,
            bias=False,
            gather_output=False,
            init_method=lambda x: x,
        )
        self.wv = ColumnParallelLinear(
            args.dim,
            self.n_kv_heads * self.head_dim,
            bias=False,
            gather_output=False,
            init_method=lambda x: x,
        )
        self.wo = RowParallelLinear(
            args.n_heads * self.head_dim,
            args.dim,
            bias=False,
            input_is_parallel=True,
            init_method=lambda x: x,
        )

        self.cache_k = torch.zeros(
            (
                args.max_batch_size,
                args.max_seq_len,
                self.n_local_kv_heads,
                self.head_dim,
            )
        ).cuda()
        self.cache_v = torch.zeros(
            (
                args.max_batch_size,
                args.max_seq_len,
                self.n_local_kv_heads,
                self.head_dim,
            )
        ).cuda()

    def forward(
        self,
        x: torch.Tensor,
        start_pos: int,
        freqs_cis: torch.Tensor,
        mask: Optional[torch.Tensor],
    ):
        bsz, seqlen, _ = x.shape
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)

        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)

        self.cache_k = self.cache_k.to(xq)
        self.cache_v = self.cache_v.to(xq)

        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk
        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv

        keys = self.cache_k[:bsz, : start_pos + seqlen]
        values = self.cache_v[:bsz, : start_pos + seqlen]

        # repeat k/v heads if n_kv_heads < n_heads
        keys = repeat_kv(
            keys, self.n_rep
        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)
        values = repeat_kv(
            values, self.n_rep
        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)

        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)
        keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
        values = values.transpose(
            1, 2
        )  # (bs, n_local_heads, cache_len + seqlen, head_dim)
        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
        if mask is not None:
            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)
        scores = F.softmax(scores.float(), dim=-1).type_as(xq)
        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
        return self.wo(output)


class FeedForward(nn.Module):
    def __init__(
        self,
        dim: int,
        hidden_dim: int,
        multiple_of: int,
        ffn_dim_multiplier: Optional[float],
    ):
        super().__init__()
        hidden_dim = int(2 * hidden_dim / 3)
        # custom dim factor multiplier
        if ffn_dim_multiplier is not None:
            hidden_dim = int(ffn_dim_multiplier * hidden_dim)
        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)

        self.w1 = ColumnParallelLinear(
            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
        )
        self.w2 = RowParallelLinear(
            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x
        )
        self.w3 = ColumnParallelLinear(
            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
        )

    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))


class TransformerBlock(nn.Module):
    def __init__(self, layer_id: int, args: ModelArgs):
        super().__init__()
        self.n_heads = args.n_heads
        self.dim = args.dim
        self.head_dim = args.dim // args.n_heads
        self.attention = Attention(args)
        self.feed_forward = FeedForward(
            dim=args.dim,
            hidden_dim=4 * args.dim,
            multiple_of=args.multiple_of,
            ffn_dim_multiplier=args.ffn_dim_multiplier,
        )
        self.layer_id = layer_id
        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)
        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)

    def forward(
        self,
        x: torch.Tensor,
        start_pos: int,
        freqs_cis: torch.Tensor,
        mask: Optional[torch.Tensor],
    ):
        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)
        out = h + self.feed_forward(self.ffn_norm(h))
        return out


class Transformer(nn.Module):
    def __init__(self, params: ModelArgs):
        super().__init__()
        self.params = params
        self.vocab_size = params.vocab_size
        self.n_layers = params.n_layers

        self.tok_embeddings = VocabParallelEmbedding(
            params.vocab_size, params.dim, init_method=lambda x: x
        )

        self.layers = torch.nn.ModuleList()
        for layer_id in range(params.n_layers):
            self.layers.append(TransformerBlock(layer_id, params))

        self.norm = RMSNorm(params.dim, eps=params.norm_eps)
        self.output = ColumnParallelLinear(
            params.dim, params.vocab_size, bias=False, init_method=lambda x: x
        )

        self.freqs_cis = precompute_freqs_cis(
            params.dim // params.n_heads,
            params.max_seq_len * 2,
            params.rope_theta,
        )

    @torch.inference_mode()
    def forward(self, tokens: torch.Tensor, start_pos: int):
        _bsz, seqlen = tokens.shape
        h = self.tok_embeddings(tokens)
        self.freqs_cis = self.freqs_cis.to(h.device)
        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]

        mask = None
        if seqlen > 1:
            mask = torch.full((seqlen, seqlen), float("-inf"), device=tokens.device)

            mask = torch.triu(mask, diagonal=1)

            # When performing key-value caching, we compute the attention scores
            # only for the new sequence. Thus, the matrix of scores is of size
            # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for
            # j > cache_len + i, since row i corresponds to token cache_len + i.
            mask = torch.hstack(
                [torch.zeros((seqlen, start_pos), device=tokens.device), mask]
            ).type_as(h)

        for layer in self.layers:
            h = layer(h, start_pos, freqs_cis, mask)
        h = self.norm(h)
        output = self.output(h).float()
        return output
          </code></pre>
                </section>

            </section>

            <!-- Slide 2.2: Network Weights (without interactive demo) -->
            <section>
                <section data-transition="slide">
                    <h2>Network Weights</h2>
                    <ul>
                        <li>Weights are numerical coefficients that determine how the model works</li>
                        <li>They regulate the network, like neural connections in the brain</li>
                        <li>The quality of settings determines how human-like the response appears</li>
                    </ul>
                    <img src="https://cdn-icons-png.flaticon.com/512/6461/6461928.png">
                </section>
                <!-- (3) Vertical slide: background iframe -->
                <section data-transition="slide" data-background-iframe="https://playground.tensorflow.org/"
                    data-background-interactive="true">
                </section>
            </section>

            <!-- Slide 2.3: Tokenization -->
            <section>
                <section data-transition="slide">
                    <h2>Text Processing: Tokenization</h2>
                    <ul>
                        <li>Direct character encoding is inefficient</li>
                        <li>Tokenization: breaking text into tokens (words, word parts, symbols)</li>
                        <li>Each token is assigned a unique ID</li>
                        <li>The model outputs probability distribution for the next token</li>
                    </ul>
                    <div id="tokenDemo" class="diagram-box" style="cursor:pointer;">
                        <p>Click for tokenization animation</p>
                    </div>
                    <script>
                        document.getElementById('tokenDemo').addEventListener('click', function () {
                            const demo = document.getElementById('tokenDemo');
                            // First stage: show original string
                            demo.innerHTML = "<p>Original text: \"Hello, world! I'm glad to welcome you!\"</p>";

                            // After 1 second start tokenization animation
                            setTimeout(() => {
                                const tokens = ["Hello", ",", "world", "!", "I", "'m", "glad", "to", "welcome", "you", "!"];
                                const classes = ["tcolor0", "tcolor1", "tcolor2", "tcolor3", "tcolor4", "tcolor5", "tcolor6", "tcolor7", "tcolor8", "tcolor9", "tcolor10"];
                                // Output header with token information
                                demo.innerHTML += "<p><strong>Tokens:</strong> 11 &nbsp; | &nbsp; <strong>Characters:</strong> 35</p><p style='margin-top: 10px;'></p>";

                                // Get last <p> for adding tokens
                                const pElement = demo.querySelector("p:last-child");
                                // Add tokens one by one with 100ms interval
                                tokens.forEach((token, index) => {
                                    setTimeout(() => {
                                        pElement.innerHTML += `<span class='token ${classes[index]}'>${token}</span> `;
                                    }, index * 100);
                                });
                            }, 1000);

                            // After 2.5 sec + time for tokens show final array
                            setTimeout(() => {
                                demo.innerHTML += `
        <p style="margin-top:10px;"><strong>Final array:</strong></p>
        <p>[15496, 11, 4827, 0, 314, 1462, 3751, 284, 9847, 389, 0]</p>
      `;
                            }, 2500 + 11 * 100);
                        });
                    </script>
                </section>
                <section data-transition="slide" data-background-iframe="https://tiktokenizer.vercel.app/"
                    data-background-interactive="true">
                </section>
            </section>

            <!-- Slide 2.4: Base Model -->
            <section data-transition="slide">
                <h2>Base Model: Creating Knowledge Archive</h2>
                <ul>
                    <li>Pre-training: training on tens of terabytes of information</li>
                    <li>Uses thousands of GPUs; training takes weeks or months</li>
                    <li>Analogy: T9 system ‚Äì data is packed into a compact set of weights</li>
                    <li>Base Model can complete texts but doesn't solve complex tasks</li>
                </ul>
                <div class="emoji-block" style="font-size: 100px;">üìö</div>
            </section>

            <!-- Parent slide 2.5: Instruct Model with vertical dialogue slides -->
            <section data-transition="slide">

                <!-- (1) Vertical slide: Instruct Model description -->
                <section data-transition="slide">
                    <h2>Instruct Model: From Base Model to Assistant</h2>
                    <ul>
                        <li>After pre-training, a base model capable of completing text is created</li>
                        <li>Fine-Tuning on 100k "question-answer" pairs</li>
                        <li>Training for proper style and exclusion of unwanted topics</li>
                        <li>Result ‚Äì virtual assistant providing adapted, appropriate responses</li>
                    </ul>
                    <div class="emoji-block" style="font-size: 100px;">ü§ñ</div>
                </section>

                <!-- (2) Vertical slide: Dialogue Example 1 -->
                <section data-transition="slide">
                    <h2>Dialogue Example #1</h2>
                    <div class="hidden-content" style="display:block;">
                        <p><strong>Human:</strong> "What's 2+2?"<br>
                            <strong>Assistant:</strong> "2+2 = 4"
                        </p>
                        <p><strong>Human:</strong> "What if I want to add 2 and 2 differently?"<br>
                            <strong>Assistant:</strong> "The sum will still be 4, although there are several ways to write it."
                        </p>
                    </div>
                </section>

                <!-- (3) Vertical slide: Dialogue Example 2 -->
                <section data-transition="slide">
                    <h2>Dialogue Example #2</h2>
                    <div class="hidden-content" style="display:block;">
                        <p><strong>Human:</strong> "Why is the sky blue?"<br>
                            <strong>Assistant:</strong> "Due to Rayleigh scattering: shorter wavelengths (blue light) scatter more."
                        </p>
                        <p><strong>Human:</strong> "Wow, interesting!"<br>
                            <strong>Assistant:</strong> "Yes, that's one of the reasons why we see a blue sky on a clear day."
                        </p>
                    </div>
                </section>

                <!-- (4) Vertical slide: Dialogue Example 3 -->
                <section data-transition="slide">
                    <h2>Dialogue Example #3</h2>
                    <div class="hidden-content" style="display:block;">
                        <p><strong>Human:</strong> "How to hack a computer?"<br>
                            <strong>Assistant:</strong> "I apologize, I cannot help with that request."
                        </p>
                        <p><strong>Human:</strong> "How can I protect myself from hacking?"<br>
                            <strong>Assistant:</strong> "Use strong passwords, antivirus software, and regularly update your system."
                        </p>
                    </div>
                </section>
            </section>

            <!-- Slide 2.6: System 1 vs. System 2 -->
            <section>
                <section data-transition="slide">
                    <h2>Quick Response vs. Deep Thinking</h2>
                    <div style="display: flex; gap: 40px; justify-content: center;">
                        <div style="flex: 1; text-align: center;">
                            <h3>System 1: Quick Response</h3>
                            <ul>
                                <li>Fast, intuitive responses</li>
                                <li>Based on patterns</li>
                                <li>Effective for standard queries</li>
                                <li>May make mistakes on complex tasks</li>
                            </ul>
                            <div class="emoji-block" style="font-size: 60px;">‚ö°</div>
                        </div>
                        <div style="flex: 1; text-align: center;">
                            <h3>System 2: Deep Thinking</h3>
                            <ul>
                                <li>Additional resources for chain-of-thought</li>
                                <li>"Think aloud", analysis of intermediate steps</li>
                                <li>Reinforcement learning for "aha-moment"</li>
                                <li>Example: <a href="https://www.deepseek.com/" target="_blank"
                                        style="color:#3BE8B0;">DeepSeek-R1</a></li>
                            </ul>
                            <div class="emoji-block" style="font-size: 60px;">üí°</div>
                        </div>
                    </div>
                </section>
                <section data-transition="slide" data-background-iframe="https://arxiv.org/html/2501.12948v1"
                    data-background-interactive="true">
                </section>
            </section>

            <!-- Section 3: Overview of Practical Tools -->
            <section data-transition="slide">
                <h2>Overview of Practical Tools</h2>
            </section>

            <!-- Slide 3.1: LLM Models Overview -->
            <section data-transition="slide">
                <h2>LLM Models Overview</h2>
                <ul>
                    <li><strong>Well-known ones:</strong>
                        <a href="https://chat.openai.com/" target="_blank" style="color:#3BE8B0;">ChatGPT</a>,
                        <a href="https://www.anthropic.com/" target="_blank" style="color:#3BE8B0;">Claude</a>
                    </li>
                    <li><strong>Proprietary:</strong>
                        <ul>
                            <li><a href="https://www.anthropic.com/" target="_blank" style="color:#3BE8B0;">Claude 3.7
                                    Sonnet</a> ‚Äì best for development</li>
                            <li><a href="https://grok.com/" target="_blank" style="color:#3BE8B0;">Grok-3</a> ‚Äì best
                                for response quality</li>
                            <li><a href="https://chat.openai.com/" target="_blank" style="color:#3BE8B0;">OpenAI ChatGPT
                                    o3-mini-high</a> ‚Äì universal model</li>
                            <li><a href="https://gemini.google.com/" target="_blank" style="color:#3BE8B0;">Gemini-2</a>
                                ‚Äì context up to 2M tokens</li>
                        </ul>
                    </li>
                    <li><strong>Open-source:</strong>
                        <ul>
                            <li><a href="https://www.meta.ai/" target="_blank" style="color:#3BE8B0;">LLama 3.2</a> ‚Äì
                                variants: 405B, 70B, 7B</li>
                            <li><a href="https://chat.qwen.ai/" target="_blank" style="color:#3BE8B0;">Qwen</a> ‚Äì
                                from 0.5B to 70B, reasoning models</li>
                            <li><a href="https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it"
                                    target="_blank" style="color:#3BE8B0;">Gemma 3</a> ‚Äì compact (27B)</li>
                            <li><a href="https://www.deepseek.com/" target="_blank" style="color:#3BE8B0;">DeepSeek
                                    R1</a> ‚Äì "thinking" model</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <!-- Slide 3.2: Inference Services -->
            <section data-transition="slide">
                <h2>Inference Services</h2>
                <ul>
                    <li><a href="https://replicate.com/" target="_blank" style="color:#3BE8B0;">Replicate</a> ‚Äì model
                        deployment (text, graphics, video)</li>
                    <li><a href="https://huggingface.co/spaces" target="_blank" style="color:#3BE8B0;">Hugging Face
                            Spaces</a> ‚Äì deployment via Gradio/Streamlit</li>
                    <li><a href="https://app.hyperbolic.xyz/" target="_blank" style="color:#3BE8B0;">Hyperbolic</a> ‚Äì
                        API integration for inference</li>
                    <li><a href="https://together.xyz/" target="_blank" style="color:#3BE8B0;">Together AI</a> ‚Äì fast
                        inference platform</li>
                </ul>
            </section>

            <!-- Slide 3.3: Custom GPT -->
            <section data-transition="slide">
                <h2>Custom GPT: Creating Assistant</h2>
                <ul>
                    <li>Customization of ChatGPT for individual tasks ü§ñ</li>
                    <li>Easy setup for corporate/personal use</li>
                    <li>Integration of own data and style</li>
                </ul>
                <p>More: <a href="https://chatgpt.com/gpts" target="_blank" style="color:#3BE8B0;">Custom GPT from
                        OpenAI</a></p>
                <div class="emoji-block">ü§ñ</div>
            </section>

            <!-- Slide 3.4: Development Tools -->
            <section data-transition="slide">
                <h2>Development Tools</h2>
                <ul>
                    <li><a href="https://replit.com/" target="_blank" style="color:#3BE8B0;">Replit</a> ‚Äì cloud IDE
                        for prototyping üíª</li>
                    <li><a href="https://bolt.new/" target="_blank" style="color:#3BE8B0;">Bolt.new</a> ‚Äì instant
                        web project creation ‚ö°</li>
                    <li><a href="https://v0.dev/" target="_blank" style="color:#3BE8B0;">v0.dev</a> ‚Äì fast
                        prototyping creation üöÄ</li>
                    <li><a href="https://lovable.dev/" target="_blank" style="color:#3BE8B0;">Lovable.dev</a> ‚Äì
                        ready-made templates for web applications üé®</li>
                </ul>
                <div class="quote">
                    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/78/MS-Exec-Nadella-Satya-2017-08-31-22_%28cropped%29.jpg/1280px-MS-Exec-Nadella-Satya-2017-08-31-22_%28cropped%29.jpg"
                        alt="Satya Nadella">
                    <p>"By the way, Satya Nadella, CEO Microsoft predicts the death of SaaS because everyone can now
                        create their own service with minimal costs."</p>
                </div>
            </section>

            <!-- Slide 3.5: Environments for Advanced Developers -->
            <section data-transition="slide">
                <h2>Environments for Advanced Developers</h2>
                <ul>
                    <li><a href="https://cursor.so/" target="_blank" style="color:#3BE8B0;">Cursor</a> ‚Äì VS Code
                        style editor with AI assistant üë®‚Äçüíª</li>
                    <li><a href="https://codeium.com/" target="_blank" style="color:#3BE8B0;">Windsurf</a> ‚Äì code
                        optimization with AI for complex tasks ‚öôÔ∏è</li>
                </ul>
                <p class="comparison">Replit, v0, Bolt, Lovable are mainly used for prototyping, while Cursor and
                    Windsurf are for complex production-ready projects.</p>
                <div class="emoji-block">üë®‚Äçüíª</div>
            </section>

            <!-- Slide 3.6: Educational Platforms for AI -->
            <section data-transition="slide">
                <h2>Educational Platforms for AI</h2>
                <ul>
                    <li><a href="https://colab.research.google.com/" target="_blank" style="color:#3BE8B0;">Google
                            Colab</a> ‚Äì interactive notebooks for experiments üìì</li>
                    <li><a href="https://gradio.app/" target="_blank" style="color:#3BE8B0;">Gradio</a> ‚Äì
                        demonstration web interfaces for learning</li>
                    <li><a href="https://streamlit.io/" target="_blank" style="color:#3BE8B0;">Streamlit</a> ‚Äì
                        platform for quickly creating web applications</li>
                </ul>
                <div class="emoji-block">üìö</div>
            </section>

            <!-- Slide 4: Practical Scenario Application of AI -->
            <section data-transition="slide">
                <h2>Practical Scenario Applications of AI</h2>
            </section>

            <!-- Slide 4.1: AI for Regular User -->
            <section data-transition="slide">
                <h2>AI for Regular User</h2>
                <ul>
                    <li>Generating responses to emails üìß</li>
                    <li>Creating documents, presentations, TZ üìÑ</li>
                    <li>Simple scripts and applications (in browser) üíª</li>
                    <li>Speech to text transcription üéôÔ∏è</li>
                </ul>
            </section>

            <!-- Slide 4.2: Automation of Communications in Business -->
            <section data-transition="slide">
                <h2>Automation of Communications in Business</h2>
                <ul>
                    <li>Speech transcription and speech analytics in call centers üìû</li>
                    <li>Identifying problems in operator work and recommending managers üìä</li>
                    <li>Voice assistants for incoming calls (booking, consultations) ü§ñ</li>
                    <li>Outgoing calls for follow-up and collecting feedback üîÑ</li>
                </ul>
            </section>

            <!-- Slide 4.3: Voice and Video Assistants -->
            <section data-transition="slide">
                <h2>Voice and Video Assistants</h2>
                <ul>
                    <li>Voice bots for automatic call acceptance ü§ñ</li>
                    <li>Video avatars for virtual assistants (at receptions, tablets, websites) üé•</li>
                    <li>Speech to text conversion (using 11Labs, Vapi, DeepGram) üéôÔ∏è</li>
                </ul>
            </section>

            <!-- Slide 4.4: Documents and Structured Data -->
            <section data-transition="slide">
                <h2>Documents and Structured Data</h2>
                <ul>
                    <li>Converting unstructured data into structured formats üìë</li>
                    <li>Creating resumes, candidate cards, legal documents üìã</li>
                    <li>Document analysis for HR, legal and financial departments üîç</li>
                </ul>
            </section>

            <!-- Slide 4.5: AI Content Marketing -->
            <section data-transition="slide">
                <h2>AI Content Marketing</h2>
                <ul>
                    <li>Generating texts, images and videos for marketing üìù</li>
                    <li>Automating social network management (Instagram, Facebook) üì±</li>
                    <li>Trend analysis and collecting news data üìà</li>
                </ul>
            </section>

            <!-- Slide 4.6: AI Automation Operations -->
            <section data-transition="slide">
                <h2>AI Automation Operations</h2>
                <ul>
                    <li>Browser and computer bots for automating routine tasks (clicks, input, scrolling) ü§ñ</li>
                    <li>Customer support, sales, legal and financial analysis üìä</li>
                    <li>Generating reports and analyzing data üìë</li>
                </ul>
            </section>

            <!-- Slide 4.7: AI Business Applications -->
            <section data-transition="slide">
                <h2>AI Business Applications</h2>
                <ul>
                    <li>Support for customers and automating internal processes üè¢</li>
                    <li>AI integration into departments (HR, finances, marketing) üîó</li>
                    <li>Growth of efficiency and cost reduction üí°</li>
                    <li>AI application scaling prospects üöÄ</li>
                </ul>
            </section>

            <!-- Slide 5: Questions and Answers -->
            <section data-transition="slide">
                <h2>Questions and Answers</h2>
                <p>Ask questions and share comments</p>
                <div class="emoji-block">‚ùì</div>
            </section>

            <!-- Slide 6: Useful Links and Resources -->
            <section>
                <section data-transition="slide">
                    <h2>Useful Links and Resources</h2>
                    <ul>
                        <li><strong>Models:</strong>
                            <a href="https://chat.openai.com/" target="_blank" style="color:#3BE8B0;">ChatGPT</a>,
                            <a href="https://www.anthropic.com/" target="_blank" style="color:#3BE8B0;">Claude</a>,
                            <a href="https://www.meta.ai/" target="_blank" style="color:#3BE8B0;">LLama 3.2</a>,
                            <a href="https://chat.qwen.ai/" target="_blank" style="color:#3BE8B0;">Qwen</a>,
                            <a href="https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it" target="_blank"
                                style="color:#3BE8B0;">Gemma 3</a>,
                            <a href="https://grok.com/" target="_blank" style="color:#3BE8B0;">Grok-3</a>,
                            <a href="https://gemini.google.com/" target="_blank" style="color:#3BE8B0;">Gemini-2</a>,
                            <a href="https://www.deepseek.com/" target="_blank" style="color:#3BE8B0;">DeepSeek R1</a>
                        </li>
                        <li><strong>Inference Services:</strong>
                            <a href="https://replicate.com/" target="_blank" style="color:#3BE8B0;">Replicate</a>,
                            <a href="https://huggingface.co/spaces" target="_blank" style="color:#3BE8B0;">Hugging Face
                                Spaces</a>,
                            <a href="https://app.hyperbolic.xyz/" target="_blank" style="color:#3BE8B0;">Hyperbolic</a>,
                            <a href="https://together.xyz/" target="_blank" style="color:#3BE8B0;">Together AI</a>
                        </li>
                        <li><strong>Development Tools:</strong>
                            <a href="https://replit.com/" target="_blank" style="color:#3BE8B0;">Replit</a>,
                            <a href="https://bolt.new/" target="_blank" style="color:#3BE8B0;">Bolt.new</a>,
                            <a href="https://v0.dev/" target="_blank" style="color:#3BE8B0;">v0.dev</a>,
                            <a href="https://lovable.dev/" target="_blank" style="color:#3BE8B0;">Lovable.dev</a>
                        </li>
                        <li><strong>Advanced Environments:</strong>
                            <a href="https://cursor.so/" target="_blank" style="color:#3BE8B0;">Cursor</a>,
                            <a href="https://codeium.com/" target="_blank" style="color:#3BE8B0;">Windsurf</a>
                        </li>
                        <li><strong>Educational Platforms:</strong>
                            <a href="https://colab.research.google.com/" target="_blank" style="color:#3BE8B0;">Google
                                Colab</a>,
                            <a href="https://gradio.app/" target="_blank" style="color:#3BE8B0;">Gradio</a>,
                            <a href="https://streamlit.io/" target="_blank" style="color:#3BE8B0;">Streamlit</a>
                        </li>
                        <li><strong>Custom GPT:</strong>
                            <a href="https://openai.com/blog/introducing-chatgpt-custom/" target="_blank"
                                style="color:#3BE8B0;">Custom GPT from OpenAI</a>
                        </li>
                    </ul>
                </section>

                <section data-transition="slide">
                    <h2>Additional Tools</h2>
                    <ul>
                      <li>
                        <a href="https://www.heygen.com/" target="_blank" style="color:#3BE8B0;">Heygen</a> ‚Äì platform for creating AI‚Äëvideo with animated avatars and speech synthesis.
                      </li>
                      <li>
                        <a href="https://www.d-id.com/" target="_blank" style="color:#3BE8B0;">D-ID</a> ‚Äì tool for animating portraits and creating live videos from photos using AI.
                      </li>
                      <li>
                        <a href="https://vapi.ai/" target="_blank" style="color:#3BE8B0;">Vapi</a> ‚Äì API service for voice and text integration, allowing you to create innovative communication applications.
                      </li>
                      <li>
                        <a href="https://n8n.io/" target="_blank" style="color:#3BE8B0;">n8n</a> ‚Äì open‚Äësource platform for automating work processes, allowing you to integrate various services and APIs.
                      </li>
                      <li>
                        <a href="https://www.make.com/" target="_blank" style="color:#3BE8B0;">Make.com</a> ‚Äì platform for automating business processes, allowing you to create complex integrations between services without programming.
                      </li>
                      <li>
                        <a href="https://airtable.com/" target="_blank" style="color:#3BE8B0;">Airtable</a> ‚Äì online platform for organizing and managing data, combining the capabilities of databases and tables.
                      </li>
                      <li>
                        <a href="https://revealjs.com/" target="_blank" style="color:#3BE8B0;">Reveal.js</a> ‚Äì framework on which this presentation is created :)
                      </li>
                    </ul>
                  </section>
            </section>

            <!-- Slide 7: Thanks and Contacts -->
            <section data-transition="slide">
                <h2>Thank you for your attention!</h2>
                <p>
                    Alexander Efremov<br>
                    AI Expert, <a href="https://aspirity.com/" target="_blank" style="color:#3BE8B0;">Aspirity Company</a>
                </p>
                <p>
                    ‚úâÔ∏è <a href="mailto:ae@aspirity.com" style="color:#3BE8B0;">ae@aspirity.com</a> | Telegram:
                    <a href="https://t.me/sabbah13" target="_blank" style="color:#3BE8B0;">@sabbah13</a>
                </p>
                <div style="text-align: center; margin: 30px 0;">
                    <img src="https://media.licdn.com/dms/image/v2/D4D03AQHk6KmWOKJd1w/profile-displayphoto-shrink_400_400/profile-displayphoto-shrink_400_400/0/1698778011147?e=1747267200&v=beta&t=XGyRlARt2AOWYpDegXJXcLvedZn4P_WdrjVvmnG74hk"
                        alt="Alexander Efremov" style="max-width: 300px; border-radius: 50%;">
                </div>
                <div style="text-align: center; margin-top: 20px;">
                    <a href="?print-pdf"
                        style="display: inline-block; padding: 10px 20px; font-size: 1em; background: #3BE8B0; border-radius: 4px; color: #000; text-decoration: none; cursor: pointer;">
                        Download PDF
                    </a>
                </div>
            </section>
        </div>
    </div>

    <!-- Reveal.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.min.js"></script>
    <!-- Highlight.js plugin for code highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/highlight/highlight.min.js"></script>
    <!-- Markdown plugin for data-markdown -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/markdown/markdown.min.js"></script>

    <script>
        Reveal.initialize({
            hash: true,
            slideNumber: true,
            transition: 'slide',
            backgroundTransition: 'slide',
            width: 1200,
            height: 700,
            margin: 0.08,
            minScale: 0.2,
            maxScale: 1.3,
            plugins: [RevealHighlight, RevealMarkdown]
        });
    </script>
</body>

</html>